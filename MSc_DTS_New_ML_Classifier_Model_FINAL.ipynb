{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MSc_DTS_New_ML_Classifier_Model",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hblacksmith/Clustering/blob/main/MSc_DTS_New_ML_Classifier_Model_FINAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwK5-9FIB-lu"
      },
      "source": [
        "# Natural Language Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1kiO9kACE6s"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QG7sxmoCIvN"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "execution_count": 631,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTfaCIzdCLPA"
      },
      "source": [
        "## Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv('AppData2.csv', encoding='ISO-8859–1')"
      ],
      "metadata": {
        "id": "9tG_1g7Cq9yk"
      },
      "execution_count": 632,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qekztq71CixT"
      },
      "source": [
        "## Cleaning the texts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8u_yXh9dCmEE",
        "outputId": "5fc9f970-564f-433f-af6f-ef6040242590",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "corpus = []\n",
        "for i in range(0, 100):\n",
        "  text = re.sub('[^a-zA-Z]', ' ', dataset['Text'][i])\n",
        "  text = text.lower()\n",
        "  text = text.split()\n",
        "  ps = PorterStemmer()\n",
        "  all_stopwords = stopwords.words('english')\n",
        "  all_stopwords.remove('not')\n",
        "  text = [ps.stem(word) for word in text if not word in set(all_stopwords)]\n",
        "  text = ' '.join(text)\n",
        "  corpus.append(text)"
      ],
      "execution_count": 633,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpGWdrzGoAsL"
      },
      "source": [
        "# View all words in corpus\n",
        "#print(corpus)"
      ],
      "execution_count": 634,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLqmAkANCp1-"
      },
      "source": [
        "## Creating the Bag of Words model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qroF7XcSCvY3"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "cv = TfidfVectorizer(max_features = 1500)\n",
        "X = cv.fit_transform(corpus).toarray()\n",
        "y = dataset.iloc[:, -1].values"
      ],
      "execution_count": 635,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DH_VjgPzC2cd"
      },
      "source": [
        "## Splitting the dataset into the Training set and Test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQXYM5VzDDDI"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)"
      ],
      "execution_count": 636,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkIq23vEDIPt"
      },
      "source": [
        "## Training the Naive Bayes model on the Training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DS9oiDXXDRdI"
      },
      "source": [
        "#Classifier: GaussianNB\n",
        "#from sklearn.naive_bayes import GaussianNB\n",
        "#classifier = GaussianNB()\n",
        "#classifier.fit(X_train, y_train)"
      ],
      "execution_count": 637,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Classifier: MultinomialNB\n",
        "#from sklearn.naive_bayes import MultinomialNB\n",
        "#classifier = MultinomialNB()\n",
        "#classifier.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "uvKF71tC6euK"
      },
      "execution_count": 638,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Classifier: SVM\n",
        "#from sklearn.svm import SVC\n",
        "#classifier = SVC(kernel = 'linear', random_state = 0)\n",
        "#classifier.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "cTfSPtyDvtGt"
      },
      "execution_count": 639,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Classifier: Kernel SVM\n",
        "#from sklearn.svm import SVC\n",
        "#classifier = SVC(kernel = 'rbf', random_state = 0)\n",
        "#classifier.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "5-pxmnfYWPDV"
      },
      "execution_count": 640,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Classifier: K-NN\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
        "classifier.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "DX68Mhy3VrCu",
        "outputId": "dfe9ad37-0397-4129-e363-4d5880b6ec3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 641,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KNeighborsClassifier()"
            ]
          },
          "metadata": {},
          "execution_count": 641
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Classifier: Random Forest \n",
        "#from sklearn.ensemble import RandomForestClassifier\n",
        "#classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
        "#classifier.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "Ez4fDzJMV5OT"
      },
      "execution_count": 642,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JaRM7zXDWUy"
      },
      "source": [
        "## Predicting the Test set results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iif0CVhFDaMp",
        "outputId": "7680c868-f2a1-4735-9536-b2c1c3102e61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y_pred = classifier.predict(X_test)\n",
        "print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))"
      ],
      "execution_count": 643,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [1 1]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [1 1]\n",
            " [0 0]\n",
            " [1 1]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 1]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 1]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoMltea5Dir1"
      },
      "source": [
        "## Measuring Performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xj9IU6MxDnvo",
        "outputId": "feb98cf6-23ae-46ed-a587-968e73cf322b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Confusion Matrix - Accuracy\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "accuracy_score(y_test, y_pred)\n",
        "#TL = TN\n",
        "#TR = FN\n",
        "#BL = FP\n",
        "#BR = TP\n"
      ],
      "execution_count": 644,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[25  0]\n",
            " [ 2  3]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9333333333333333"
            ]
          },
          "metadata": {},
          "execution_count": 644
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#What the model got right\n",
        "y_true = np.array([0,0,0,0,0,1,0,0,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0])\n",
        "y_pred = np.array([0,0,0,0,0,1,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0])\n",
        "true_positives = ((y_pred == y_true) & (y_pred == 1)).sum()\n",
        "true_positives"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rv0K2v7X3knJ",
        "outputId": "2dc8eeb9-b904-4df0-c027-d953ab950d90"
      },
      "execution_count": 645,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 645
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "true_negatives = ((y_pred == y_true) & (y_pred == 0)).sum()\n",
        "true_negatives"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jr8sgsfv3rIe",
        "outputId": "63b8bc44-fbec-4065-fcdf-2b2401941501"
      },
      "execution_count": 646,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25"
            ]
          },
          "metadata": {},
          "execution_count": 646
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#What the model got wrong\n",
        "false_positives = ((y_pred != y_true) & (y_pred == 1)).sum()\n",
        "false_positives"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDBB-WpQ3vuE",
        "outputId": "ae80d2f8-7de6-4002-bebb-2619c4c9496c"
      },
      "execution_count": 647,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 647
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "false_negatives = ((y_pred != y_true) & (y_pred == 0)).sum()\n",
        "false_negatives"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1A9dnjh3xbS",
        "outputId": "24ab7631-5b8e-4e16-fe0c-d776531743eb"
      },
      "execution_count": 648,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 648
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Precision - how many true labels did the model detect (1.0 = NO FALSE POSITIVES)\n",
        "precision = true_positives / (true_positives + false_positives)\n",
        "precision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxtjxSeL30ge",
        "outputId": "35b1ca49-d4cd-453d-f6de-f41417d33f80"
      },
      "execution_count": 649,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 649
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Recall - how many positive examples were detected in the dataset\n",
        "recall = true_positives / (true_positives + false_negatives)\n",
        "recall"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPpWx-iT32EJ",
        "outputId": "9a9028e3-1e26-46fe-9f88-31e0dc4fa7df"
      },
      "execution_count": 650,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6"
            ]
          },
          "metadata": {},
          "execution_count": 650
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#RMSE - Root Mean Square Error\n",
        "rmse = np.sqrt((y_true - y_pred) ** 2 / len(y_true))\n",
        "rmse"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roPa56G733wK",
        "outputId": "244049e6-02b9-4b68-9d15-5441bc9a95d5"
      },
      "execution_count": 651,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.18257419,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.18257419,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ])"
            ]
          },
          "metadata": {},
          "execution_count": 651
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZcOJH_x9GSV"
      },
      "source": [
        "## Predicting if a single statement is Pass [1] or Reject [0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CiVD7lB-G4E"
      },
      "source": [
        "### Positive review"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-jZEzWD49su",
        "outputId": "294aa3a5-647d-46d4-f58b-e2c9b003acdf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "new_statement = \"When leading a team, I am proudest when I see my direct reports develop and start working autonomously to a high standard. An example was when I worked at the charity Retail Trust as Head of Marketing & Insights and inherited a junior marketing insights team where I needed to upskill them in modern ways of data driven marketing. They had had no formal training or individual objectives. I worked with team members individually on an annual learning and development plan, covering strengths and development areas. Recognising the value of feedback, I met with everyone separately bi-weekly to track progress and discuss any training needs. Each month I got the whole team together and asked them to showcase what new projects they were working on using their new skills and how that contributed to the charity’s vision of supporting an extra 20,000 people a year with wellbeing in the retail sector. After a few months they had built up practical knowledge on data-led decision making, influential communications and proving return on investment. I arranged for them to present to the heads of department explaining what they had learnt about those most in need in the retail sector, any new skills and actions the charity should take.  Consequently, the CEO took notice and asked my team to begin working with other areas of the business in digital upskilling. What I valued most was the praise and achievements the team received as they grew and successfully helped the charity deliver its vision.\"\n",
        "new_statement = re.sub('[^a-zA-Z]', ' ', new_statement)\n",
        "new_statement = new_statement.lower()\n",
        "new_statement = new_statement.split()\n",
        "ps = PorterStemmer()\n",
        "all_stopwords = stopwords.words('english')\n",
        "all_stopwords.remove('not')\n",
        "new_statement = [ps.stem(word) for word in new_statement if not word in set(all_stopwords)]\n",
        "new_statement = ' '.join(new_statement)\n",
        "new_corpus = [new_statement]\n",
        "new_X_test = cv.transform(new_corpus).toarray()\n",
        "new_y_pred = classifier.predict(new_X_test)\n",
        "print(new_y_pred)"
      ],
      "execution_count": 657,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rkorcoxm93kP"
      },
      "source": [
        "The review was correctly predicted as positive by our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qztXT-i-K1j"
      },
      "source": [
        "### Negative review"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1DsvzmVE9EK"
      },
      "source": [
        "**Solution:** We just repeat the same text preprocessing process we did before, but this time with a single review."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySAblyR98J2s",
        "outputId": "4c42d242-f00b-4a4f-8822-741aa65f262a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "new_statement = \"As the MOD's Head of Workforce Analytics for Strategic Command I analysed the military and civilian workforce data across ten departments including Digital, Intelligence, Special Forces and Cyber. When the annual task of reviewing critical gaps in the workforce was commissioned by Head Office I sought to work with the departments in creating their analysis instead of issuing a spreadsheet and waiting for each HR team to return it. I knew UKStratCom's strategic areas were cyber, special forces, intelligence and interoperability enabled through technology. I analysed over 25k employee records to identify the strength and establishment of those strategic roles and identified shortfalls. Next, I reached out to the finance and workforce teams of each department and presented my data, aiming for the following outcomes: 1) Understanding and agreeing the workforce data 2) Assessing the significance of the critical workforce gaps 3) Discussing mitigating actions to address the gaps 4) Agreeing when the gaps would be resolved Insufficient recruitment was a dominant factor and in many instances the Army were unable to fill their posts. I wanted to dig further and get their view for a rounded analysis. I shared my findings with the Army’s workforce analytics team who recognised the affect their recruitment pipeline was having on my Command's outputs and they described the actions they were taking. By reaching out to a wide selection of stakeholders I built a comprehensive view of the problem and delivered a robust analysis which I included on the Integrated Review paper.\"\n",
        "new_statement = re.sub('[^a-zA-Z]', ' ', new_statement)\n",
        "new_statement = new_statement.lower()\n",
        "new_statement = new_statement.split()\n",
        "ps = PorterStemmer()\n",
        "all_stopwords = stopwords.words('english')\n",
        "all_stopwords.remove('not')\n",
        "new_statement = [ps.stem(word) for word in new_statement if not word in set(all_stopwords)]\n",
        "new_statement = ' '.join(new_statement)\n",
        "new_corpus = [new_statement]\n",
        "new_X_test = cv.transform(new_corpus).toarray()\n",
        "new_y_pred = classifier.predict(new_X_test)\n",
        "print(new_y_pred)"
      ],
      "execution_count": 658,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d72teliY-PUQ"
      },
      "source": [
        "The review was correctly predicted as negative by our model."
      ]
    }
  ]
}